{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a14b3f-8ca0-48c3-b61c-21c38ec5cfb8",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc5f0ce-0871-40a3-b00d-2d469ba64225",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sales = \"select ([Time].[Day] * [Version].[Version Name].[CurrentWorkingView]  * [Department].[Department_ID] * [Store].[Store_ID].[1] * {Measure.[Weekly Sales]}  ) on row, () on column;\"\n",
    "_features = \"select( [Time].[Day] * [Version].[Version Name].[CurrentWorkingView] * [Store].[Store_ID].[1] * { Measure.[Temperature], Measure.[Fuel Price], Measure.[MarkDown1] , Measure.[MarkDown2], Measure.[MarkDown3], Measure.[MarkDown4] , Measure.[MarkDown5] , Measure.[CPI] , Measure.[Unemployment] } ) on row, () on column;\"\n",
    "_stores = \"select([Store].[Store_ID].[1] * [Store].[Type] ) on row, () on column;\"\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\n",
    "O9DataLake.register(\"sales\",DataSource.LS, ResourceType.IBPL, _sales)\n",
    "O9DataLake.register(\"features\",DataSource.LS, ResourceType.IBPL, _features)\n",
    "O9DataLake.register(\"stores\",DataSource.LS, ResourceType.IBPL, _stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e87970-813a-4630-817e-89295bbeba08",
   "metadata": {},
   "outputs": [
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-784cdd5fc694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mstores_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store.[Store_ID]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Store.[Type]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_x_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-784cdd5fc694>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(sales, features, stores)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CPI'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CPI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CPI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unemployment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unemployment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unemployment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Temperature'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Fuel Price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MarkDown3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/qarelease_m5predictplugintest/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean requires at least one data point'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exec plugin instance [DemoPlugin] for measures {[PredictedSales]} using scope ([Version].[Version Name].[CurrentWorkingView]*Store.[Store_ID]) using arguments {(NumExecutors,1), (ExecutorMemory, \"1G\"), (DriverMemory, \"2G\")};\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger('o9_logger')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "KNN = \"KNN\"\n",
    "LINRRG = \"LINREG\"\n",
    "DECISIONTREE = \"DECISIONTREE\"\n",
    "RANDOMFOREST = \"RANDOMFOREST\"\n",
    "EXTRATREES = \"EXTRATREES\"\n",
    "available_models = [KNN, LINRRG, DECISIONTREE, RANDOMFOREST] # EXTRATREES]\n",
    "\n",
    "class o9Models:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_availabe_models(self):\n",
    "        return available_models\n",
    "\n",
    "    def get_model(self, model_name):\n",
    "        model_name = model_name.upper()\n",
    "        assert (model_name.upper() in available_models), \"can not get [{}] model\".format(model_name)\n",
    "        if model_name == KNN:\n",
    "            return self.get_knn()\n",
    "        if model_name == LINRRG:\n",
    "            return self.get_lin_reg()\n",
    "        if model_name == DECISIONTREE:\n",
    "            return self.get_dtree()\n",
    "        if model_name == RANDOMFOREST:\n",
    "            return self.get_randomForest()\n",
    "\n",
    "    def get_lin_reg(self):\n",
    "        lin_reg_model = LinearRegression()\n",
    "        return lin_reg_model\n",
    "\n",
    "    def get_knn(self):\n",
    "        knn_model = KNeighborsRegressor(n_neighbors=10,n_jobs=4)\n",
    "        return knn_model\n",
    "\n",
    "    def get_dtree(self):\n",
    "        dtree_model = DecisionTreeRegressor(random_state=0)\n",
    "        return dtree_model\n",
    "\n",
    "    def get_randomForest(self):\n",
    "        randomForest_model = RandomForestRegressor(n_estimators = 400,max_depth=15,n_jobs=5)\n",
    "        return randomForest_model\n",
    "\n",
    "    def get_xgb(self):\n",
    "        #xgb_model = XGBRegressor(objective='reg:linear', nthread=4, n_estimators=500, max_depth=6, learning_rate=0.5)\n",
    "        #return xgb_model\n",
    "        pass\n",
    "\n",
    "    def get_arima(self):\n",
    "        #from statsmodel.tsa.arima_model import ARIMA\n",
    "        #arima = ARIMA()\n",
    "        pass\n",
    "\n",
    "    def get_extratrees(self):\n",
    "        etr = ExtraTreesRegressor(n_estimators=30, n_jobs=4)\n",
    "        return etr\n",
    "\n",
    "\n",
    "def get_dataset(sales, features, stores):\n",
    "    \"\"\"\n",
    "    merges teh dataframe\n",
    "    fills the dataframe nulls with 0\n",
    "    \"\"\"\n",
    "    dataset = sales.merge(stores, how='left').merge(features, how='left')\n",
    "    from statistics import mean\n",
    "    dataset['CPI'] = dataset['CPI'].fillna(mean(dataset['CPI']))\n",
    "    dataset['Unemployment'] = dataset['Unemployment'].fillna(mean(dataset['Unemployment']))\n",
    "    dataset[['Temperature','Fuel Price','MarkDown3']] \\\n",
    "        = dataset[['Temperature','Fuel Price','MarkDown3']].fillna(0)\n",
    "    dataset[['CPI', 'Unemployment']] = dataset[['CPI', 'Unemployment']].fillna(0)\n",
    "    date = pd.to_datetime(dataset[\"Time.[Day]\"], format=\"%m/%d/%Y\")\n",
    "    dataset['Year'] = date.dt.year\n",
    "    dataset['Day'] = date.dt.day\n",
    "    dataset['Month'] = date.dt.month\n",
    "    dataset[\"Days to Next Christmas\"] = (\n",
    "                pd.to_datetime(\"12/31/\" + dataset[\"Year\"].astype(str), format=\"%m/%d/%Y\") -\n",
    "                date).dt.days.astype(int)\n",
    "    dataset = dataset.drop(columns=['MarkDown1','MarkDown2', 'MarkDown4','MarkDown5'])\n",
    "    return dataset\n",
    "\n",
    "def create_x_y(dataset):\n",
    "    \"\"\"\"\n",
    "    weekly sales is the predicted output\n",
    "    rest of the columns are input features\n",
    "    \"\"\"\n",
    "    X = dataset.loc[:, dataset.columns != 'Weekly Sales']\n",
    "    X = pd.get_dummies(X, columns=[\"Store.[Type]\"])\n",
    "    y = dataset[['Weekly Sales']]\n",
    "    return (X, y)\n",
    "\n",
    "def drop_columns(X):\n",
    "    return X.drop(columns =['Time.[Day]'])\n",
    "\n",
    "def scale_x_y(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    from sklearn import preprocessing\n",
    "    sc_X = preprocessing.StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def build_model_and_get_metrics(model_name, X_train, X_test, y_train, y_test):\n",
    "    models = o9Models()\n",
    "    model = models.get_model(model_name)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn import metrics\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    model_metrics = {\n",
    "        \"MeanAbsoluteError\" : mae,\n",
    "        \"MeanSquaredError\" : mse,\n",
    "        \"RootMeanSquaredError\" : rmse,\n",
    "        \"Accuracy\" : accuracy\n",
    "    }\n",
    "    return model, model_metrics\n",
    "\n",
    "def run_a_model(model_name, X_train, X_test, y_train, y_test):\n",
    "    logger.info(\"building {} model...\".format(model_name))\n",
    "    model, metrics = build_model_and_get_metrics(model_name, X_train, X_test, y_train, y_test)\n",
    "    details = {\"model\": model, \"metrics\": metrics}\n",
    "    logger.info(details)\n",
    "    return details\n",
    "\n",
    "\n",
    "def run_models(X_train, X_test, y_train, y_test):\n",
    "    out = {}\n",
    "    logger.info(\"started model building\")\n",
    "    for model_name in available_models:\n",
    "        model_details = run_a_model(model_name, X_train, X_test, y_train, y_test)\n",
    "        out[model_name] = model_details\n",
    "    logger.info(out)\n",
    "\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake\n",
    "sales = O9DataLake.get(\"sales\")\n",
    "features = O9DataLake.get(\"features\")\n",
    "stores = O9DataLake.get(\"stores\")\n",
    "\n",
    "sales_df = sales[['Time.[Day]', 'Department.[Department_ID]','Store.[Store_ID]','Weekly Sales']]\n",
    "features_df = features[['Store.[Store_ID]','Time.[Day]', 'Temperature','Fuel Price','MarkDown1','MarkDown2','MarkDown3', 'MarkDown4','MarkDown5', 'CPI', 'Unemployment']]\n",
    "stores_df = stores[['Store.[Store_ID]', 'Store.[Type]']]\n",
    "\n",
    "dataset = get_dataset(sales_df, features_df, stores_df)\n",
    "(X, y) = create_x_y(dataset)\n",
    "X = drop_columns(X)\n",
    "X_train, X_test, y_train, y_test = scale_x_y(X, y)\n",
    "#run_models(X_train, X_test, y_train, y_test)\n",
    "run_a_model(DECISIONTREE, X_train, X_test, y_train, y_test)\n",
    "logger.info(\"output df\")\n",
    "Output = sales.head(5)\n",
    "O9DataLake.put(\"Output\", Output)\n",
    "logger.info(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf7c4d-0b4f-40d7-be0c-eb87a975c535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[m5PredictPluginTest] Tenant Conda Environment",
   "language": "python",
   "name": "qarelease_m5predictplugintest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notebook_dict": {
   "ClassName": "o9.GraphCube.Plugins.Python.PythonScript",
   "InstanceName": "DemoPlugin",
   "SliceKeys": [],
   "file_path": "loaded_notebooks/DemoPlugin.ipynb",
   "o9_selected_plugin_id": 13511
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
